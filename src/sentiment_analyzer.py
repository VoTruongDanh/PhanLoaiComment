"""
Tool ph√¢n t√≠ch sentiment cho comments TikTok
ƒê√°nh gi√° c·ªôt text v√† t·∫°o c·ªôt trust:
- 1: t√≠ch c·ª±c (positive)
- 0: trung t√≠nh (neutral)
- -1: ti√™u c·ª±c (negative)
"""

import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import pipeline
import torch
from tqdm import tqdm
import warnings
import sys
import os
import re

# Fix encoding for Windows console
if sys.platform == 'win32':
    try:
        # Set console to UTF-8
        os.system('chcp 65001 >nul 2>&1')
        sys.stdout.reconfigure(encoding='utf-8') if hasattr(sys.stdout, 'reconfigure') else None
        sys.stderr.reconfigure(encoding='utf-8') if hasattr(sys.stderr, 'reconfigure') else None
    except:
        pass

warnings.filterwarnings('ignore')

# T·ª´ kh√≥a t√≠ch c·ª±c ti·∫øng Vi·ªát
POSITIVE_KEYWORDS = [
    'xinh', 'ƒë·∫πp', 'cute', 'd·ªÖ th∆∞∆°ng', 'hay', 't·ªët', 'tuy·ªát', 'vui', 'th√≠ch', 'y√™u',
    'love', 'amazing', 'great', 'good', 'nice', 'beautiful', 'wonderful', 'awesome',
    'th√∫ v·ªã', 'h√†i', 'vui v·∫ª', 'h·∫°nh ph√∫c', 'tuy·ªát v·ªùi', 'xu·∫•t s·∫Øc', 'gi·ªèi', 't√†i',
    'khen', 'khen ng·ª£i', '·ªßng h·ªô', 'ƒë·ªìng √Ω', 'ƒë√∫ng', 'ch√≠nh x√°c', 'chu·∫©n', 'ok', 'okay',
    'kh√¥ng sao', 'k sao', 'ko sao', '·ªïn', 'ok', 'fine', 'alright'
]

# T·ª´ kh√≥a ti√™u c·ª±c ti·∫øng Vi·ªát
NEGATIVE_KEYWORDS = [
    'x·∫•u', 't·ªá', 'd·ªü', 't·ªìi', 'k√©m', 'gh√©t', 'ch√°n', 'bu·ªìn', 'th·∫•t v·ªçng', 't·ª©c',
    'bad', 'terrible', 'awful', 'hate', 'disgusting', 'horrible', 'worst', 'stupid',
    'ngu', 'd·ªët', 'ƒë·∫ßn', 'l∆∞·ªùi', 'v√¥ d·ª•ng', 'ph·∫£n ƒë·ªëi', 'sai', 'kh√¥ng ƒë√∫ng',
    'ch√™', 'ph√™ ph√°n', 'ch·ªâ tr√≠ch', 't·ª©c gi·∫≠n', 'b·ª±c', 'kh√≥ ch·ªãu'
]

# Emoji t√≠ch c·ª±c
POSITIVE_EMOJIS = ['üòä', 'üòç', 'ü•∞', 'üòò', 'üòÅ', 'üòÇ', 'ü§ó', 'üòÑ', 'üòÉ', 'üòÜ', 'üòâ', 
                   'üíï', 'üíñ', 'üíó', 'üíì', 'üíû', '‚ù§Ô∏è', 'üß°', 'üíõ', 'üíö', 'üíô', 
                   'üíú', 'ü§ç', 'üñ§', 'ü§é', 'üíØ', 'üëç', 'üëè', 'üéâ', 'üéä', '‚ú®', 'üåü']

# Emoji ti√™u c·ª±c
NEGATIVE_EMOJIS = ['üò¢', 'üò≠', 'üò§', 'üò†', 'üò°', 'ü§¨', 'üòû', 'üòî', 'üòü', 'üòï', 
                   'üôÅ', '‚òπÔ∏è', 'üò£', 'üòñ', 'üò´', 'üò©', 'üíî', 'üëé', '‚ùå', 'üö´']


class SentimentAnalyzer:
    """Ph√¢n t√≠ch sentiment s·ª≠ d·ª•ng model ƒëa ng√¥n ng·ªØ"""
    
    def __init__(self, model_name='cardiffnlp/twitter-roberta-base-sentiment-latest'):
        """
        Kh·ªüi t·∫°o sentiment analyzer
        
        Args:
            model_name: T√™n model t·ª´ HuggingFace
                       - 'cardiffnlp/twitter-roberta-base-sentiment-latest': Nhanh, h·ªó tr·ª£ ƒëa ng√¥n ng·ªØ
                       - 'nlptown/bert-base-multilingual-uncased-sentiment': Ch√≠nh x√°c h∆°n, ch·∫≠m h∆°n
        """
        print(f"ƒêang t·∫£i model: {model_name}...")
        self.device = 0 if torch.cuda.is_available() else -1
        try:
            self.sentiment_pipeline = pipeline(
                "sentiment-analysis",
                model=model_name,
                tokenizer=model_name,
                device=self.device,
                return_all_scores=False,
                truncation=True,
                max_length=512
            )
            print(f"Model ƒë√£ s·∫µn s√†ng (device: {'GPU' if self.device >= 0 else 'CPU'})")
        except Exception as e:
            print(f"L·ªói khi t·∫£i model: {e}")
            print("ƒêang th·ª≠ model d·ª± ph√≤ng...")
            # Fallback to multilingual model
            self.sentiment_pipeline = pipeline(
                "sentiment-analysis",
                model='nlptown/bert-base-multilingual-uncased-sentiment',
                device=self.device,
                return_all_scores=False,
                truncation=True,
                max_length=512
            )
            print("ƒê√£ t·∫£i model d·ª± ph√≤ng th√†nh c√¥ng")
    
    def _check_keywords_and_emojis(self, text):
        """
        Ki·ªÉm tra t·ª´ kh√≥a v√† emoji ƒë·ªÉ b·ªï sung cho ph√¢n t√≠ch sentiment
        
        Returns:
            tuple: (positive_score, negative_score) t·ª´ 0-1
        """
        text_lower = text.lower()
        positive_score = 0
        negative_score = 0
        
        # Ki·ªÉm tra t·ª´ kh√≥a t√≠ch c·ª±c
        for keyword in POSITIVE_KEYWORDS:
            if keyword in text_lower:
                positive_score += 0.3
        
        # Ki·ªÉm tra t·ª´ kh√≥a ti√™u c·ª±c
        for keyword in NEGATIVE_KEYWORDS:
            if keyword in text_lower:
                negative_score += 0.3
        
        # Ki·ªÉm tra emoji t√≠ch c·ª±c
        for emoji in POSITIVE_EMOJIS:
            if emoji in text:
                positive_score += 0.2
        
        # Ki·ªÉm tra emoji ti√™u c·ª±c
        for emoji in NEGATIVE_EMOJIS:
            if emoji in text:
                negative_score += 0.2
        
        # X·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát
        if any(phrase in text_lower for phrase in ['kh√¥ng sao', 'k sao', 'ko sao', 'khong sao']):
            positive_score += 0.5
        
        if any(phrase in text_lower for phrase in ['haha', 'hihi', 'hehe']):
            positive_score += 0.3
        
        return min(positive_score, 1.0), min(negative_score, 1.0)
    
    def analyze_text(self, text):
        """
        Ph√¢n t√≠ch sentiment cho m·ªôt ƒëo·∫°n text
        
        Args:
            text: ƒêo·∫°n text c·∫ßn ph√¢n t√≠ch
            
        Returns:
            int: 1 (positive), 0 (neutral), -1 (negative)
        """
        if pd.isna(text) or not str(text).strip():
            return 0
        
        try:
            # Gi·ªõi h·∫°n ƒë·ªô d√†i text ƒë·ªÉ tr√°nh l·ªói
            text = str(text)[:512]
            
            # Ki·ªÉm tra t·ª´ kh√≥a v√† emoji tr∆∞·ªõc
            pos_keyword_score, neg_keyword_score = self._check_keywords_and_emojis(text)
            
            # Ph√¢n t√≠ch b·∫±ng model
            result = self.sentiment_pipeline(text)[0]
            label = result['label'].upper()
            score = result.get('score', 0.5)
            
            # Chuy·ªÉn ƒë·ªïi label th√†nh trust score
            model_score = 0
            if '5 STAR' in label or '4 STAR' in label:
                model_score = 1
            elif '1 STAR' in label or '2 STAR' in label:
                model_score = -1
            elif 'POSITIVE' in label or 'POS' in label:
                model_score = 1
            elif 'NEGATIVE' in label or 'NEG' in label:
                model_score = -1
            
            # K·∫øt h·ª£p k·∫øt qu·∫£ model v·ªõi t·ª´ kh√≥a/emoji
            final_score = model_score
            
            # ƒê·∫øm s·ªë emoji t√≠ch c·ª±c v√† ti√™u c·ª±c
            pos_emoji_count = sum(1 for emoji in POSITIVE_EMOJIS if emoji in text)
            neg_emoji_count = sum(1 for emoji in NEGATIVE_EMOJIS if emoji in text)
            
            # N·∫øu c√≥ nhi·ªÅu emoji t√≠ch c·ª±c, ∆∞u ti√™n t√≠ch c·ª±c
            if pos_emoji_count >= 2 and model_score <= 0:
                final_score = 1
            # N·∫øu c√≥ emoji t√≠ch c·ª±c v√† t·ª´ kh√≥a t√≠ch c·ª±c, ∆∞u ti√™n t√≠ch c·ª±c
            elif pos_emoji_count >= 1 and pos_keyword_score > 0.3 and model_score <= 0:
                final_score = 1
            # N·∫øu c√≥ nhi·ªÅu emoji ti√™u c·ª±c, ∆∞u ti√™n ti√™u c·ª±c
            elif neg_emoji_count >= 2 and model_score >= 0:
                final_score = -1
            # N·∫øu t·ª´ kh√≥a/emoji m·∫°nh, ƒëi·ªÅu ch·ªânh k·∫øt qu·∫£
            elif pos_keyword_score > 0.5 and model_score <= 0:
                final_score = 1  # ∆Øu ti√™n t√≠ch c·ª±c n·∫øu c√≥ nhi·ªÅu d·∫•u hi·ªáu t√≠ch c·ª±c
            elif neg_keyword_score > 0.5 and model_score >= 0:
                final_score = -1  # ∆Øu ti√™n ti√™u c·ª±c n·∫øu c√≥ nhi·ªÅu d·∫•u hi·ªáu ti√™u c·ª±c
            elif pos_keyword_score > neg_keyword_score + 0.3:
                final_score = 1
            elif neg_keyword_score > pos_keyword_score + 0.3:
                final_score = -1
            
            return final_score
                
        except Exception as e:
            print(f"L·ªói khi ph√¢n t√≠ch: {text[:50]}... - {str(e)}")
            return 0
    
    def analyze_batch(self, texts, batch_size=32, progress_callback=None):
        """
        Ph√¢n t√≠ch sentiment cho nhi·ªÅu texts (nhanh h∆°n)
        
        Args:
            texts: List ho·∫∑c Series c√°c texts
            batch_size: S·ªë l∆∞·ª£ng texts x·ª≠ l√Ω c√πng l√∫c
            progress_callback: H√†m callback ƒë·ªÉ c·∫≠p nh·∫≠t progress (current, total)
            
        Returns:
            numpy array: M·∫£ng c√°c trust scores
        """
        results = []
        total_batches = (len(texts) + batch_size - 1) // batch_size
        
        # X·ª≠ l√Ω theo batch ƒë·ªÉ tƒÉng t·ªëc
        # S·ª≠ d·ª•ng tqdm n·∫øu kh√¥ng c√≥ callback (cho CLI)
        if progress_callback is None:
            try:
                progress_range = tqdm(range(0, len(texts), batch_size), desc="Ph√¢n t√≠ch sentiment")
            except:
                progress_range = range(0, len(texts), batch_size)
        else:
            progress_range = range(0, len(texts), batch_size)
        
        for batch_idx, i in enumerate(progress_range):
            if progress_callback:
                progress_callback(batch_idx + 1, total_batches)
            batch = texts[i:i+batch_size].tolist()
            
            # X·ª≠ l√Ω t·ª´ng text trong batch ƒë·ªÉ tr√°nh l·ªói
            for text in batch:
                if pd.isna(text) or not str(text).strip():
                    results.append(0)
                    continue
                
                try:
                    # S·ª≠ d·ª•ng analyze_text ƒë·ªÉ c√≥ logic c·∫£i thi·ªán
                    trust_score = self.analyze_text(text)
                    results.append(trust_score)
                except Exception as e:
                    # N·∫øu l·ªói, m·∫∑c ƒë·ªãnh l√† neutral
                    results.append(0)
        
        return np.array(results)
    
    def process_csv(self, input_file, output_file=None, text_column='text', trust_column='trust', batch_size=32):
        """
        X·ª≠ l√Ω file CSV: ƒë·ªçc, ph√¢n t√≠ch sentiment, v√† l∆∞u k·∫øt qu·∫£
        
        Args:
            input_file: ƒê∆∞·ªùng d·∫´n file CSV ƒë·∫ßu v√†o
            output_file: ƒê∆∞·ªùng d·∫´n file CSV ƒë·∫ßu ra (n·∫øu None th√¨ ghi ƒë√® file ƒë·∫ßu v√†o)
            text_column: T√™n c·ªôt ch·ª©a text
            trust_column: T√™n c·ªôt trust c·∫ßn t·∫°o/c·∫≠p nh·∫≠t
            batch_size: S·ªë l∆∞·ª£ng texts x·ª≠ l√Ω c√πng l√∫c
        """
        print(f"ƒêang ƒë·ªçc file: {input_file}")
        df = pd.read_csv(input_file)
        
        print(f"T·ªïng s·ªë d√≤ng: {len(df)}")
        print(f"C·ªôt text c√≥ {df[text_column].notna().sum()} gi√° tr·ªã kh√¥ng r·ªóng")
        
        # Ki·ªÉm tra xem c·ªôt trust ƒë√£ t·ªìn t·∫°i ch∆∞a
        if trust_column not in df.columns:
            df[trust_column] = None
        
        # L·ªçc c√°c d√≤ng c·∫ßn ph√¢n t√≠ch (ch∆∞a c√≥ trust ho·∫∑c trust r·ªóng)
        mask = df[trust_column].isna() | (df[trust_column] == '')
        texts_to_analyze = df.loc[mask, text_column]
        
        if len(texts_to_analyze) == 0:
            print("T·∫•t c·∫£ c√°c d√≤ng ƒë√£ c√≥ trust score. Kh√¥ng c·∫ßn ph√¢n t√≠ch th√™m.")
            return df
        
        print(f"S·ªë d√≤ng c·∫ßn ph√¢n t√≠ch: {len(texts_to_analyze)}")
        
        # Ph√¢n t√≠ch sentiment
        print("B·∫Øt ƒë·∫ßu ph√¢n t√≠ch sentiment...")
        trust_scores = self.analyze_batch(texts_to_analyze, batch_size=batch_size)
        
        # C·∫≠p nh·∫≠t c·ªôt trust
        df.loc[mask, trust_column] = trust_scores
        
        # Th·ªëng k√™ k·∫øt qu·∫£
        print("\n=== Th·ªëng k√™ k·∫øt qu·∫£ ===")
        print(f"T√≠ch c·ª±c (1): {(df[trust_column] == 1).sum()} ({((df[trust_column] == 1).sum() / len(df) * 100):.2f}%)")
        print(f"Trung t√≠nh (0): {(df[trust_column] == 0).sum()} ({((df[trust_column] == 0).sum() / len(df) * 100):.2f}%)")
        print(f"Ti√™u c·ª±c (-1): {(df[trust_column] == -1).sum()} ({((df[trust_column] == -1).sum() / len(df) * 100):.2f}%)")
        
        # L∆∞u file
        if output_file is None:
            output_file = input_file
        
        print(f"\nƒêang l∆∞u k·∫øt qu·∫£ v√†o: {output_file}")
        df.to_csv(output_file, index=False, encoding='utf-8-sig')
        print("Ho√†n th√†nh!")
        
        return df
    
    def process_csv_dataframe(self, df, text_column='text', trust_column='trust', batch_size=32):
        """
        X·ª≠ l√Ω DataFrame tr·ª±c ti·∫øp: ph√¢n t√≠ch sentiment v√† th√™m c·ªôt trust
        
        Args:
            df: DataFrame c·∫ßn x·ª≠ l√Ω
            text_column: T√™n c·ªôt ch·ª©a text
            trust_column: T√™n c·ªôt trust c·∫ßn t·∫°o/c·∫≠p nh·∫≠t
            batch_size: S·ªë l∆∞·ª£ng texts x·ª≠ l√Ω c√πng l√∫c
            
        Returns:
            DataFrame: DataFrame ƒë√£ ƒë∆∞·ª£c th√™m c·ªôt trust
        """
        # Ki·ªÉm tra xem c·ªôt trust ƒë√£ t·ªìn t·∫°i ch∆∞a
        if trust_column not in df.columns:
            df[trust_column] = None
        
        # L·ªçc c√°c d√≤ng c·∫ßn ph√¢n t√≠ch (ch∆∞a c√≥ trust ho·∫∑c trust r·ªóng)
        mask = df[trust_column].isna() | (df[trust_column] == '')
        texts_to_analyze = df.loc[mask, text_column]
        
        if len(texts_to_analyze) == 0:
            return df
        
        # L·∫•y progress callback n·∫øu c√≥
        progress_callback = getattr(self, 'progress_callback', None)
        
        # Ph√¢n t√≠ch sentiment
        trust_scores = self.analyze_batch(texts_to_analyze, batch_size=batch_size, progress_callback=progress_callback)
        
        # C·∫≠p nh·∫≠t c·ªôt trust
        df.loc[mask, trust_column] = trust_scores
        
        return df


def main():
    """H√†m main ƒë·ªÉ ch·∫°y tool"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Ph√¢n t√≠ch sentiment cho comments TikTok')
    parser.add_argument('--input', '-i', 
                       default='dataset_tiktok-comments-637video-scraper_2026-01-15.csv',
                       help='File CSV ƒë·∫ßu v√†o')
    parser.add_argument('--output', '-o', 
                       default=None,
                       help='File CSV ƒë·∫ßu ra (n·∫øu kh√¥ng ch·ªâ ƒë·ªãnh th√¨ ghi ƒë√® file ƒë·∫ßu v√†o)')
    parser.add_argument('--model', '-m',
                       default='cardiffnlp/twitter-roberta-base-sentiment-latest',
                       choices=['cardiffnlp/twitter-roberta-base-sentiment-latest',
                               'nlptown/bert-base-multilingual-uncased-sentiment'],
                       help='Model sentiment analysis')
    parser.add_argument('--batch-size', '-b',
                       type=int, default=32,
                       help='K√≠ch th∆∞·ªõc batch (m·∫∑c ƒë·ªãnh: 32)')
    parser.add_argument('--text-column', '-t',
                       default='text',
                       help='T√™n c·ªôt ch·ª©a text (m·∫∑c ƒë·ªãnh: text)')
    parser.add_argument('--trust-column', '-c',
                       default='trust',
                       help='T√™n c·ªôt trust (m·∫∑c ƒë·ªãnh: trust)')
    
    args = parser.parse_args()
    
    # Kh·ªüi t·∫°o analyzer
    analyzer = SentimentAnalyzer(model_name=args.model)
    
    # X·ª≠ l√Ω file
    analyzer.process_csv(
        input_file=args.input,
        output_file=args.output,
        text_column=args.text_column,
        trust_column=args.trust_column,
        batch_size=args.batch_size
    )


if __name__ == '__main__':
    main()
